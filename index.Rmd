---
title: "Practical Machine Learning Course Project"
author: "Eric Jung"
date: "December 26, 2015"
output: html_document
---

**Description:** This is the report summary of the course project for the Practical Machine Learning course offered by JHU on Coursera for the 12/6/2015-1/3/2016 session. A training and testing data set are offered from the Human Activity Recognition Weight Lifting Activities website at <http://groupware.les.inf.puc-rio.br/har>. The data sets offer a 5-level classification from 6 different subjects of the quality of a weight lifting exercise from "A" to "E" with 159 variables per observation,  including some record-keeping info (e.g. timestamps). The goal of the project is to create a predictive model that can accurately classify the weight lifting exercise measurements from the testing data set (20 total samples)


```{r,echo=FALSE, warning=FALSE, message=FALSE}
# libraries
library(caret)
library(rpart)
library(randomForest)
library(ggplot2)
library(lattice)

# read in training data set
training <- read.csv("./pml-training.csv")
testing <- read.csv("./pml-testing.csv")

# check for two types of predictors in training set for removal
#   1.  majority of values are missing (90%)
#   2.  any values are missing (either blank, div/0, or NA)

removelist = c(1,2,3,4,5) # list of column numbers to remove due to dominant NA/blank + some extras
incompletelist = numeric() # list of remaining columns with missing values for Imputation if necessary
for(i in 1 : dim(training)[2]){
  countNA = sum(is.na(training[,i]))
  countBlank = sum(na.omit(training[,i]) == "")
  countDiv0 = sum(na.omit(training[,i]) == "#DIV/0")
  if((countNA + countBlank + countDiv0)/dim(training)[1] > .9){
    removelist = c(removelist, i)
  }
  else if((countNA + countBlank + countDiv0) > 0){
    incompletelist = c(incompletelist,i)    
  }
}

nsvlist <-  nearZeroVar(training)
removelist <- unique(c(removelist,nsvlist))

## remove data from remove list
training_postprocess = training[,-removelist]
testing_postprocess = testing[,-removelist]

## modelfit and prediction
modFit <- randomForest(classe ~ ., training_postprocess, ntree=100, norm.votes=FALSE)
pred_training <- predict(modFit, training_postprocess)
pred_testing <- predict(modFit, testing_postprocess)

```

## Feature Investigation and Covariate Determination
Using *qplot()*, several different plots are obtained comparing the training set outcome variable **classe** and various observation variables. The following is one such example, showing the number of outcomes color-coded by the subject names. The figure shows that the number of outcomes does not seem particularly weighted towards any particular subject:

```{r, echo=FALSE}
qplot(classe, colour = user_name, data = training)
```

The user_name, therefore, was not considered as part of the covariate set.

However, the most revealing trend came from the *summary()* function on the training data. This revealed that several of the observations being recorded were dominantly unreported, e.g. with blanks, NA, or error (DIV/0) values, which were removed. Using the *nearZeroVar()* function, we also remove the observations with very little variance. The following variables were removed from the training set as a result of these two observations:

```{r, echo = FALSE}
names(training)[removelist]
```

The number of observations per record was therefore reduced from 159 to 53 on which to train the predictive model. Some principle component analysis was also performed showing that only 80% of variation could be obtained with 13 covariates, but this was ultimately abandoned in the final model as it did not lead to any obvious benefit in model accuracy or computational efficiency. Ultimately, all 53 remaining variables were used as features to the predictive model. The testing set was also processed to remove all but the remaining 53 variables identical to the training set. We also note that the 53 remaining variables had **no missing values**, and therefore no imputation was necessary to fill in the missing values and no samples were removed, leaving 19622 samples in the training set with which to train the model.

## Model Creation

A random forest model fit was chosen. With the 53 different variables remaining, there was great potential for non-linearity in the model, and along with the recommendation for accuracy from the random forest lecture, it seemed to be the most obvious first choice. Due to the long runtime of using the caret *train* version for random forest, the *randomForest* library was included, and *randomForest()* function used instead. The randomForest is set to grow 100 trees based on sampling with replacement (i.e. bootstrapping), and the model summary is as follows:

```{r, echo = FALSE}
print(modFit)
```

Note that the predicted out-of-band error here is a miniscule 0.16%.

## Prediction and Comparison with the Original Training Matrix

Using the modelfit (variable name *modFit*), we obtain the predictions for all 19622 samples, and create a confusion matrix with the original predictions. 

```{r}
pred_training <- predict(modFit, training_postprocess)
confusionMatrix(pred_training,training$classe)
```

As can be seen, the confusionMatrix results in a **100%** match between the predicted value and the training set outcome. Finally, we use the *predict* command to obtain the predictions for the testing set. This resulted in a 100% match in the project submission.

```{r, echo = FALSE}
pred_testing <- predict(modFit, testing_postprocess)
pred_testing
```

